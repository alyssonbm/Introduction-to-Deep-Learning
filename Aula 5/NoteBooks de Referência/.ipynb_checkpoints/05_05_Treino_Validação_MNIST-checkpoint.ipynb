{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3R2-_YuQT1r"
   },
   "source": [
    "# Regressão Softmax com dados do MNIST utilizando gradiente descendente estocástico por minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkfGTqMVQT1u"
   },
   "source": [
    "O objetivo deste notebook é ilustrar \n",
    "- o uso do gradiente estocástico por mini-batchs\n",
    "- utilizando as classes Dataset e DataLoater.\n",
    "\n",
    "A apresentação da perda nos gráficos é um pouco diferente da usual, mostrando a perda de cada um dos vários minibatches dentro de cada época, de forma que as épocas são apresentadas com valores fracionários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6tHIO8O-ePC"
   },
   "source": [
    "## Inicializando o Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtKt7Ey7-htu",
    "outputId": "dfd40958-d6f8-43fc-9391-fef1b5c9d4c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neptune-client==0.9.1 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (0.18.2)\n",
      "Requirement already satisfied: websocket-client>=0.35.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (0.58.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (20.9)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (7.1.2)\n",
      "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (7.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (1.15.0)\n",
      "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (3.1.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (2.23.0)\n",
      "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (1.3.0)\n",
      "Requirement already satisfied: PyJWT in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (2.0.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (1.1.5)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (1.24.3)\n",
      "Requirement already satisfied: bravado in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (11.0.3)\n",
      "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (3.1.14)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client==0.9.1) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client==0.9.1) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client==0.9.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client==0.9.1) (2.10)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client==0.9.1) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client==0.9.1) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client==0.9.1) (1.19.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client==0.9.1) (3.13)\n",
      "Requirement already satisfied: monotonic in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client==0.9.1) (1.6)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client==0.9.1) (1.0.2)\n",
      "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client==0.9.1) (5.17.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client==0.9.1) (3.7.4.3)\n",
      "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client==0.9.1) (3.17.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client==0.9.1) (4.0.7)\n",
      "Requirement already satisfied: jsonref in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.9.1) (0.2)\n",
      "Requirement already satisfied: swagger-spec-validator>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.9.1) (2.7.3)\n",
      "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.9.1) (2.6.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client==0.9.1) (4.0.0)\n",
      "Requirement already satisfied: webcolors; extra == \"format\" in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.1) (1.11.1)\n",
      "Requirement already satisfied: strict-rfc3339; extra == \"format\" in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.1) (0.7)\n",
      "Requirement already satisfied: rfc3987; extra == \"format\" in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.1) (1.3.8)\n"
     ]
    }
   ],
   "source": [
    "! pip install neptune-client==0.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgE0kR_P-iea",
    "outputId": "ca827b53-91d5-4b71-a64c-d374955efdb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/rodrigonogueira/Aula-5-Exemplo/e/AUL1-30\n"
     ]
    }
   ],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "# Insira seu api_token para logar os resultados do treino na sua conta do Neptune.\n",
    "# Como obter seu API token do Neptune:\n",
    "# https://docs.neptune.ai/administration/security-and-privacy/how-to-find-and-set-neptune-api-token\n",
    "\n",
    "run = neptune.init(project='rodrigonogueira/Aula-5-Exemplo', api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhYzhlOTIxYy1lMGNhLTRiY2QtYTdjYi1jNWMyN2YxNzVhMTQifQ==')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFNf4RPxQT1w"
   },
   "source": [
    "## Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fLUSHaCQT1x"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rt0hAeSHsoBh"
   },
   "source": [
    "## Descobrindo se há uma GPU disponível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VrgJPbh2snHq",
    "outputId": "8ac62fc8-7ec6-4f48-eb0e-3b41c1e13d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    "   dev = \"cuda:0\"\n",
    "else: \n",
    "   dev = \"cpu\" \n",
    "print(dev)\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEMUsfJpQT11"
   },
   "source": [
    "## Dataset e dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHoQjDs_QT12"
   },
   "source": [
    "### Definição do tamanho do minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEQYUr4TQT13"
   },
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc7Rv_2BQT16"
   },
   "source": [
    "### Carregamento, criação dataset e do dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0dEKCn-QT17"
   },
   "outputs": [],
   "source": [
    "dataset_dir = '../data/'\n",
    "\n",
    "dataset_train = MNIST(dataset_dir, train=True, download=True,\n",
    "                      transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rOy9ntrQT2D"
   },
   "source": [
    "### Usando 1000 amostras do MNIST\n",
    "\n",
    "Neste exemplo utilizaremos 1000 amostras de treinamento e 1000 de validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9sXttHBTzCM"
   },
   "outputs": [],
   "source": [
    "dataset_train, dataset_valid, _ = torch.utils.data.random_split(dataset_train, [1000, 1000, 60000-1000-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w52KGYlIQT2A",
    "outputId": "97b2acdb-acf6-4a5b-fff6-2f63bd612a51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de minibatches de trenamento: 20\n",
      "Número de minibatches de validação: 20\n",
      "\n",
      "Dimensões dos dados de um minibatch: torch.Size([50, 1, 28, 28])\n",
      "Valores mínimo e máximo dos pixels:  tensor(0.) tensor(1.)\n",
      "Tipo dos dados das imagens:          <class 'torch.Tensor'>\n",
      "Tipo das classes das imagens:        <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Número de minibatches de trenamento:', len(loader_train))\n",
    "print('Número de minibatches de validação:', len(loader_valid))\n",
    "\n",
    "x_train, y_train = next(iter(loader_train))\n",
    "print(\"\\nDimensões dos dados de um minibatch:\", x_train.size())\n",
    "print(\"Valores mínimo e máximo dos pixels: \", torch.min(x_train), torch.max(x_train))\n",
    "print(\"Tipo dos dados das imagens:         \", type(x_train))\n",
    "print(\"Tipo das classes das imagens:       \", type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQA9Zg7GQT2G"
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_8Eg4h_kQT2H",
    "outputId": "cf9d9e0f-9e2e-4d90-ca40-d3d44f008450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo(\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=500, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Modelo(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Modelo, self).__init__()\n",
    "        self.dense = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28*28, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Modelo()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NHQB4wGQT2K"
   },
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqs2JhJoQT2L"
   },
   "source": [
    "### Inicialização dos parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZuYEkn_QT2M"
   },
   "outputs": [],
   "source": [
    "n_epochs = 400\n",
    "learningRate = 0.1\n",
    "\n",
    "# Utilizaremos CrossEntropyLoss como função de perda\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Gradiente descendente\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmXarXeIQT2O"
   },
   "source": [
    "### Laço de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5T_jZZPQT2P",
    "outputId": "8db482a6-961b-42c4-825f-de6deafbf677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época: 0/399 Train Loss: 0.044803 Valid Loss: 0.043072\n",
      "Época: 1/399 Train Loss: 0.039475 Valid Loss: 0.034673\n",
      "Época: 2/399 Train Loss: 0.027934 Valid Loss: 0.022443\n",
      "Época: 3/399 Train Loss: 0.018172 Valid Loss: 0.016587\n",
      "Época: 4/399 Train Loss: 0.013584 Valid Loss: 0.014129\n",
      "Época: 5/399 Train Loss: 0.010628 Valid Loss: 0.011810\n",
      "Época: 6/399 Train Loss: 0.009284 Valid Loss: 0.010609\n",
      "Época: 7/399 Train Loss: 0.008199 Valid Loss: 0.010112\n",
      "Época: 8/399 Train Loss: 0.007041 Valid Loss: 0.009699\n",
      "Época: 9/399 Train Loss: 0.006099 Valid Loss: 0.009725\n",
      "Época: 10/399 Train Loss: 0.005877 Valid Loss: 0.010597\n",
      "Época: 11/399 Train Loss: 0.005054 Valid Loss: 0.009805\n",
      "Época: 12/399 Train Loss: 0.004625 Valid Loss: 0.009107\n",
      "Época: 13/399 Train Loss: 0.003984 Valid Loss: 0.009366\n",
      "Época: 14/399 Train Loss: 0.004298 Valid Loss: 0.008792\n",
      "Época: 15/399 Train Loss: 0.003243 Valid Loss: 0.009058\n",
      "Época: 16/399 Train Loss: 0.003013 Valid Loss: 0.009168\n",
      "Época: 17/399 Train Loss: 0.002768 Valid Loss: 0.008857\n",
      "Época: 18/399 Train Loss: 0.002527 Valid Loss: 0.009417\n",
      "Época: 19/399 Train Loss: 0.002334 Valid Loss: 0.008942\n",
      "Época: 20/399 Train Loss: 0.001948 Valid Loss: 0.009314\n",
      "Época: 21/399 Train Loss: 0.002003 Valid Loss: 0.009341\n",
      "Época: 22/399 Train Loss: 0.001629 Valid Loss: 0.009611\n",
      "Época: 23/399 Train Loss: 0.001523 Valid Loss: 0.009094\n",
      "Época: 24/399 Train Loss: 0.001419 Valid Loss: 0.009333\n",
      "Época: 25/399 Train Loss: 0.001303 Valid Loss: 0.009393\n",
      "Época: 26/399 Train Loss: 0.001123 Valid Loss: 0.009772\n",
      "Época: 27/399 Train Loss: 0.001054 Valid Loss: 0.009603\n",
      "Época: 28/399 Train Loss: 0.000993 Valid Loss: 0.009904\n",
      "Época: 29/399 Train Loss: 0.000866 Valid Loss: 0.009712\n",
      "Época: 30/399 Train Loss: 0.000792 Valid Loss: 0.009652\n",
      "Época: 31/399 Train Loss: 0.000768 Valid Loss: 0.009684\n",
      "Época: 32/399 Train Loss: 0.000651 Valid Loss: 0.009829\n",
      "Época: 33/399 Train Loss: 0.000592 Valid Loss: 0.009863\n",
      "Época: 34/399 Train Loss: 0.000545 Valid Loss: 0.010016\n",
      "Época: 35/399 Train Loss: 0.000523 Valid Loss: 0.010008\n",
      "Época: 36/399 Train Loss: 0.000490 Valid Loss: 0.010069\n",
      "Época: 37/399 Train Loss: 0.000465 Valid Loss: 0.010104\n",
      "Época: 38/399 Train Loss: 0.000436 Valid Loss: 0.010183\n",
      "Época: 39/399 Train Loss: 0.000420 Valid Loss: 0.010228\n",
      "Época: 40/399 Train Loss: 0.000383 Valid Loss: 0.010245\n",
      "Época: 41/399 Train Loss: 0.000367 Valid Loss: 0.010348\n",
      "Época: 42/399 Train Loss: 0.000347 Valid Loss: 0.010361\n",
      "Época: 43/399 Train Loss: 0.000325 Valid Loss: 0.010435\n",
      "Época: 44/399 Train Loss: 0.000303 Valid Loss: 0.010546\n",
      "Época: 45/399 Train Loss: 0.000290 Valid Loss: 0.010540\n",
      "Época: 46/399 Train Loss: 0.000280 Valid Loss: 0.010586\n",
      "Época: 47/399 Train Loss: 0.000263 Valid Loss: 0.010643\n",
      "Época: 48/399 Train Loss: 0.000256 Valid Loss: 0.010704\n",
      "Época: 49/399 Train Loss: 0.000248 Valid Loss: 0.010767\n",
      "Época: 50/399 Train Loss: 0.000237 Valid Loss: 0.010870\n",
      "Época: 51/399 Train Loss: 0.000229 Valid Loss: 0.010863\n",
      "Época: 52/399 Train Loss: 0.000219 Valid Loss: 0.010875\n",
      "Época: 53/399 Train Loss: 0.000208 Valid Loss: 0.010897\n",
      "Época: 54/399 Train Loss: 0.000202 Valid Loss: 0.011018\n",
      "Época: 55/399 Train Loss: 0.000191 Valid Loss: 0.011005\n",
      "Época: 56/399 Train Loss: 0.000187 Valid Loss: 0.011056\n",
      "Época: 57/399 Train Loss: 0.000182 Valid Loss: 0.011132\n",
      "Época: 58/399 Train Loss: 0.000172 Valid Loss: 0.011127\n",
      "Época: 59/399 Train Loss: 0.000169 Valid Loss: 0.011196\n",
      "Época: 60/399 Train Loss: 0.000163 Valid Loss: 0.011209\n",
      "Época: 61/399 Train Loss: 0.000156 Valid Loss: 0.011231\n",
      "Época: 62/399 Train Loss: 0.000153 Valid Loss: 0.011276\n",
      "Época: 63/399 Train Loss: 0.000149 Valid Loss: 0.011306\n",
      "Época: 64/399 Train Loss: 0.000146 Valid Loss: 0.011358\n",
      "Época: 65/399 Train Loss: 0.000141 Valid Loss: 0.011380\n",
      "Época: 66/399 Train Loss: 0.000137 Valid Loss: 0.011443\n",
      "Época: 67/399 Train Loss: 0.000134 Valid Loss: 0.011489\n",
      "Época: 68/399 Train Loss: 0.000130 Valid Loss: 0.011505\n",
      "Época: 69/399 Train Loss: 0.000126 Valid Loss: 0.011509\n",
      "Época: 70/399 Train Loss: 0.000123 Valid Loss: 0.011562\n",
      "Época: 71/399 Train Loss: 0.000119 Valid Loss: 0.011587\n",
      "Época: 72/399 Train Loss: 0.000117 Valid Loss: 0.011606\n",
      "Época: 73/399 Train Loss: 0.000114 Valid Loss: 0.011642\n",
      "Época: 74/399 Train Loss: 0.000111 Valid Loss: 0.011659\n",
      "Época: 75/399 Train Loss: 0.000108 Valid Loss: 0.011758\n",
      "Época: 76/399 Train Loss: 0.000107 Valid Loss: 0.011726\n",
      "Época: 77/399 Train Loss: 0.000103 Valid Loss: 0.011756\n",
      "Época: 78/399 Train Loss: 0.000103 Valid Loss: 0.011788\n",
      "Época: 79/399 Train Loss: 0.000099 Valid Loss: 0.011814\n",
      "Época: 80/399 Train Loss: 0.000098 Valid Loss: 0.011822\n",
      "Época: 81/399 Train Loss: 0.000095 Valid Loss: 0.011855\n",
      "Época: 82/399 Train Loss: 0.000094 Valid Loss: 0.011915\n",
      "Época: 83/399 Train Loss: 0.000091 Valid Loss: 0.011912\n",
      "Época: 84/399 Train Loss: 0.000089 Valid Loss: 0.011950\n",
      "Época: 85/399 Train Loss: 0.000087 Valid Loss: 0.011965\n",
      "Época: 86/399 Train Loss: 0.000085 Valid Loss: 0.012042\n",
      "Época: 87/399 Train Loss: 0.000084 Valid Loss: 0.012041\n",
      "Época: 88/399 Train Loss: 0.000083 Valid Loss: 0.012038\n",
      "Época: 89/399 Train Loss: 0.000080 Valid Loss: 0.012084\n",
      "Época: 90/399 Train Loss: 0.000079 Valid Loss: 0.012101\n",
      "Época: 91/399 Train Loss: 0.000078 Valid Loss: 0.012122\n",
      "Época: 92/399 Train Loss: 0.000076 Valid Loss: 0.012125\n",
      "Época: 93/399 Train Loss: 0.000075 Valid Loss: 0.012189\n",
      "Época: 94/399 Train Loss: 0.000074 Valid Loss: 0.012184\n",
      "Época: 95/399 Train Loss: 0.000072 Valid Loss: 0.012217\n",
      "Época: 96/399 Train Loss: 0.000072 Valid Loss: 0.012258\n",
      "Época: 97/399 Train Loss: 0.000070 Valid Loss: 0.012289\n",
      "Época: 98/399 Train Loss: 0.000069 Valid Loss: 0.012286\n",
      "Época: 99/399 Train Loss: 0.000068 Valid Loss: 0.012289\n",
      "Época: 100/399 Train Loss: 0.000067 Valid Loss: 0.012311\n",
      "Época: 101/399 Train Loss: 0.000066 Valid Loss: 0.012349\n",
      "Época: 102/399 Train Loss: 0.000065 Valid Loss: 0.012342\n",
      "Época: 103/399 Train Loss: 0.000064 Valid Loss: 0.012383\n",
      "Época: 104/399 Train Loss: 0.000063 Valid Loss: 0.012400\n",
      "Época: 105/399 Train Loss: 0.000061 Valid Loss: 0.012391\n",
      "Época: 106/399 Train Loss: 0.000061 Valid Loss: 0.012445\n",
      "Época: 107/399 Train Loss: 0.000060 Valid Loss: 0.012485\n",
      "Época: 108/399 Train Loss: 0.000059 Valid Loss: 0.012459\n",
      "Época: 109/399 Train Loss: 0.000058 Valid Loss: 0.012475\n",
      "Época: 110/399 Train Loss: 0.000057 Valid Loss: 0.012498\n",
      "Época: 111/399 Train Loss: 0.000057 Valid Loss: 0.012526\n",
      "Época: 112/399 Train Loss: 0.000056 Valid Loss: 0.012552\n",
      "Época: 113/399 Train Loss: 0.000055 Valid Loss: 0.012569\n",
      "Época: 114/399 Train Loss: 0.000054 Valid Loss: 0.012585\n",
      "Época: 115/399 Train Loss: 0.000053 Valid Loss: 0.012618\n",
      "Época: 116/399 Train Loss: 0.000053 Valid Loss: 0.012631\n",
      "Época: 117/399 Train Loss: 0.000052 Valid Loss: 0.012657\n",
      "Época: 118/399 Train Loss: 0.000051 Valid Loss: 0.012649\n",
      "Época: 119/399 Train Loss: 0.000051 Valid Loss: 0.012670\n",
      "Época: 120/399 Train Loss: 0.000050 Valid Loss: 0.012693\n",
      "Época: 121/399 Train Loss: 0.000049 Valid Loss: 0.012714\n",
      "Época: 122/399 Train Loss: 0.000049 Valid Loss: 0.012739\n",
      "Época: 123/399 Train Loss: 0.000048 Valid Loss: 0.012738\n",
      "Época: 124/399 Train Loss: 0.000047 Valid Loss: 0.012753\n",
      "Época: 125/399 Train Loss: 0.000047 Valid Loss: 0.012774\n",
      "Época: 126/399 Train Loss: 0.000047 Valid Loss: 0.012787\n",
      "Época: 127/399 Train Loss: 0.000046 Valid Loss: 0.012812\n",
      "Época: 128/399 Train Loss: 0.000045 Valid Loss: 0.012819\n",
      "Época: 129/399 Train Loss: 0.000045 Valid Loss: 0.012846\n",
      "Época: 130/399 Train Loss: 0.000044 Valid Loss: 0.012856\n",
      "Época: 131/399 Train Loss: 0.000044 Valid Loss: 0.012870\n",
      "Época: 132/399 Train Loss: 0.000043 Valid Loss: 0.012891\n",
      "Época: 133/399 Train Loss: 0.000043 Valid Loss: 0.012911\n",
      "Época: 134/399 Train Loss: 0.000042 Valid Loss: 0.012913\n",
      "Época: 135/399 Train Loss: 0.000042 Valid Loss: 0.012928\n",
      "Época: 136/399 Train Loss: 0.000041 Valid Loss: 0.012953\n",
      "Época: 137/399 Train Loss: 0.000041 Valid Loss: 0.012965\n",
      "Época: 138/399 Train Loss: 0.000040 Valid Loss: 0.012974\n",
      "Época: 139/399 Train Loss: 0.000040 Valid Loss: 0.013000\n",
      "Época: 140/399 Train Loss: 0.000040 Valid Loss: 0.013016\n",
      "Época: 141/399 Train Loss: 0.000039 Valid Loss: 0.013016\n",
      "Época: 142/399 Train Loss: 0.000039 Valid Loss: 0.013021\n",
      "Época: 143/399 Train Loss: 0.000038 Valid Loss: 0.013042\n",
      "Época: 144/399 Train Loss: 0.000038 Valid Loss: 0.013061\n",
      "Época: 145/399 Train Loss: 0.000038 Valid Loss: 0.013077\n",
      "Época: 146/399 Train Loss: 0.000037 Valid Loss: 0.013090\n",
      "Época: 147/399 Train Loss: 0.000037 Valid Loss: 0.013092\n",
      "Época: 148/399 Train Loss: 0.000037 Valid Loss: 0.013104\n",
      "Época: 149/399 Train Loss: 0.000036 Valid Loss: 0.013124\n",
      "Época: 150/399 Train Loss: 0.000036 Valid Loss: 0.013146\n",
      "Época: 151/399 Train Loss: 0.000035 Valid Loss: 0.013170\n",
      "Época: 152/399 Train Loss: 0.000035 Valid Loss: 0.013180\n",
      "Época: 153/399 Train Loss: 0.000035 Valid Loss: 0.013183\n",
      "Época: 154/399 Train Loss: 0.000034 Valid Loss: 0.013195\n",
      "Época: 155/399 Train Loss: 0.000034 Valid Loss: 0.013213\n",
      "Época: 156/399 Train Loss: 0.000034 Valid Loss: 0.013217\n",
      "Época: 157/399 Train Loss: 0.000033 Valid Loss: 0.013240\n",
      "Época: 158/399 Train Loss: 0.000033 Valid Loss: 0.013240\n",
      "Época: 159/399 Train Loss: 0.000033 Valid Loss: 0.013261\n",
      "Época: 160/399 Train Loss: 0.000032 Valid Loss: 0.013271\n",
      "Época: 161/399 Train Loss: 0.000032 Valid Loss: 0.013279\n",
      "Época: 162/399 Train Loss: 0.000032 Valid Loss: 0.013290\n",
      "Época: 163/399 Train Loss: 0.000032 Valid Loss: 0.013300\n",
      "Época: 164/399 Train Loss: 0.000031 Valid Loss: 0.013320\n",
      "Época: 165/399 Train Loss: 0.000031 Valid Loss: 0.013340\n",
      "Época: 166/399 Train Loss: 0.000031 Valid Loss: 0.013344\n",
      "Época: 167/399 Train Loss: 0.000030 Valid Loss: 0.013356\n",
      "Época: 168/399 Train Loss: 0.000030 Valid Loss: 0.013367\n",
      "Época: 169/399 Train Loss: 0.000030 Valid Loss: 0.013378\n",
      "Época: 170/399 Train Loss: 0.000030 Valid Loss: 0.013394\n",
      "Época: 171/399 Train Loss: 0.000029 Valid Loss: 0.013406\n",
      "Época: 172/399 Train Loss: 0.000029 Valid Loss: 0.013410\n",
      "Época: 173/399 Train Loss: 0.000029 Valid Loss: 0.013421\n",
      "Época: 174/399 Train Loss: 0.000029 Valid Loss: 0.013438\n",
      "Época: 175/399 Train Loss: 0.000028 Valid Loss: 0.013457\n",
      "Época: 176/399 Train Loss: 0.000028 Valid Loss: 0.013459\n",
      "Época: 177/399 Train Loss: 0.000028 Valid Loss: 0.013477\n",
      "Época: 178/399 Train Loss: 0.000028 Valid Loss: 0.013481\n",
      "Época: 179/399 Train Loss: 0.000028 Valid Loss: 0.013493\n",
      "Época: 180/399 Train Loss: 0.000027 Valid Loss: 0.013495\n",
      "Época: 181/399 Train Loss: 0.000027 Valid Loss: 0.013516\n",
      "Época: 182/399 Train Loss: 0.000027 Valid Loss: 0.013526\n",
      "Época: 183/399 Train Loss: 0.000027 Valid Loss: 0.013530\n",
      "Época: 184/399 Train Loss: 0.000027 Valid Loss: 0.013541\n",
      "Época: 185/399 Train Loss: 0.000026 Valid Loss: 0.013550\n",
      "Época: 186/399 Train Loss: 0.000026 Valid Loss: 0.013567\n",
      "Época: 187/399 Train Loss: 0.000026 Valid Loss: 0.013578\n",
      "Época: 188/399 Train Loss: 0.000026 Valid Loss: 0.013593\n",
      "Época: 189/399 Train Loss: 0.000025 Valid Loss: 0.013605\n",
      "Época: 190/399 Train Loss: 0.000025 Valid Loss: 0.013605\n",
      "Época: 191/399 Train Loss: 0.000025 Valid Loss: 0.013629\n",
      "Época: 192/399 Train Loss: 0.000025 Valid Loss: 0.013636\n",
      "Época: 193/399 Train Loss: 0.000025 Valid Loss: 0.013643\n",
      "Época: 194/399 Train Loss: 0.000025 Valid Loss: 0.013649\n",
      "Época: 195/399 Train Loss: 0.000024 Valid Loss: 0.013660\n",
      "Época: 196/399 Train Loss: 0.000024 Valid Loss: 0.013678\n",
      "Época: 197/399 Train Loss: 0.000024 Valid Loss: 0.013682\n",
      "Época: 198/399 Train Loss: 0.000024 Valid Loss: 0.013688\n",
      "Época: 199/399 Train Loss: 0.000024 Valid Loss: 0.013693\n",
      "Época: 200/399 Train Loss: 0.000024 Valid Loss: 0.013713\n",
      "Época: 201/399 Train Loss: 0.000023 Valid Loss: 0.013714\n",
      "Época: 202/399 Train Loss: 0.000023 Valid Loss: 0.013733\n",
      "Época: 203/399 Train Loss: 0.000023 Valid Loss: 0.013732\n",
      "Época: 204/399 Train Loss: 0.000023 Valid Loss: 0.013751\n",
      "Época: 205/399 Train Loss: 0.000023 Valid Loss: 0.013759\n",
      "Época: 206/399 Train Loss: 0.000023 Valid Loss: 0.013760\n",
      "Época: 207/399 Train Loss: 0.000022 Valid Loss: 0.013773\n",
      "Época: 208/399 Train Loss: 0.000022 Valid Loss: 0.013778\n",
      "Época: 209/399 Train Loss: 0.000022 Valid Loss: 0.013788\n",
      "Época: 210/399 Train Loss: 0.000022 Valid Loss: 0.013804\n",
      "Época: 211/399 Train Loss: 0.000022 Valid Loss: 0.013814\n",
      "Época: 212/399 Train Loss: 0.000022 Valid Loss: 0.013819\n",
      "Época: 213/399 Train Loss: 0.000021 Valid Loss: 0.013827\n",
      "Época: 214/399 Train Loss: 0.000021 Valid Loss: 0.013842\n",
      "Época: 215/399 Train Loss: 0.000021 Valid Loss: 0.013842\n",
      "Época: 216/399 Train Loss: 0.000021 Valid Loss: 0.013853\n",
      "Época: 217/399 Train Loss: 0.000021 Valid Loss: 0.013866\n",
      "Época: 218/399 Train Loss: 0.000021 Valid Loss: 0.013873\n",
      "Época: 219/399 Train Loss: 0.000021 Valid Loss: 0.013880\n",
      "Época: 220/399 Train Loss: 0.000021 Valid Loss: 0.013885\n",
      "Época: 221/399 Train Loss: 0.000020 Valid Loss: 0.013895\n",
      "Época: 222/399 Train Loss: 0.000020 Valid Loss: 0.013902\n",
      "Época: 223/399 Train Loss: 0.000020 Valid Loss: 0.013919\n",
      "Época: 224/399 Train Loss: 0.000020 Valid Loss: 0.013928\n",
      "Época: 225/399 Train Loss: 0.000020 Valid Loss: 0.013930\n",
      "Época: 226/399 Train Loss: 0.000020 Valid Loss: 0.013937\n",
      "Época: 227/399 Train Loss: 0.000020 Valid Loss: 0.013956\n",
      "Época: 228/399 Train Loss: 0.000020 Valid Loss: 0.013961\n",
      "Época: 229/399 Train Loss: 0.000019 Valid Loss: 0.013968\n",
      "Época: 230/399 Train Loss: 0.000019 Valid Loss: 0.013978\n",
      "Época: 231/399 Train Loss: 0.000019 Valid Loss: 0.013989\n",
      "Época: 232/399 Train Loss: 0.000019 Valid Loss: 0.013993\n",
      "Época: 233/399 Train Loss: 0.000019 Valid Loss: 0.013999\n",
      "Época: 234/399 Train Loss: 0.000019 Valid Loss: 0.014015\n",
      "Época: 235/399 Train Loss: 0.000019 Valid Loss: 0.014024\n",
      "Época: 236/399 Train Loss: 0.000019 Valid Loss: 0.014024\n",
      "Época: 237/399 Train Loss: 0.000018 Valid Loss: 0.014040\n",
      "Época: 238/399 Train Loss: 0.000018 Valid Loss: 0.014038\n",
      "Época: 239/399 Train Loss: 0.000018 Valid Loss: 0.014047\n",
      "Época: 240/399 Train Loss: 0.000018 Valid Loss: 0.014056\n",
      "Época: 241/399 Train Loss: 0.000018 Valid Loss: 0.014073\n",
      "Época: 242/399 Train Loss: 0.000018 Valid Loss: 0.014071\n",
      "Época: 243/399 Train Loss: 0.000018 Valid Loss: 0.014080\n",
      "Época: 244/399 Train Loss: 0.000018 Valid Loss: 0.014089\n",
      "Época: 245/399 Train Loss: 0.000018 Valid Loss: 0.014097\n",
      "Época: 246/399 Train Loss: 0.000018 Valid Loss: 0.014103\n",
      "Época: 247/399 Train Loss: 0.000017 Valid Loss: 0.014114\n",
      "Época: 248/399 Train Loss: 0.000017 Valid Loss: 0.014122\n",
      "Época: 249/399 Train Loss: 0.000017 Valid Loss: 0.014129\n",
      "Época: 250/399 Train Loss: 0.000017 Valid Loss: 0.014136\n",
      "Época: 251/399 Train Loss: 0.000017 Valid Loss: 0.014145\n",
      "Época: 252/399 Train Loss: 0.000017 Valid Loss: 0.014153\n",
      "Época: 253/399 Train Loss: 0.000017 Valid Loss: 0.014158\n",
      "Época: 254/399 Train Loss: 0.000017 Valid Loss: 0.014165\n",
      "Época: 255/399 Train Loss: 0.000017 Valid Loss: 0.014167\n",
      "Época: 256/399 Train Loss: 0.000017 Valid Loss: 0.014178\n",
      "Época: 257/399 Train Loss: 0.000017 Valid Loss: 0.014187\n",
      "Época: 258/399 Train Loss: 0.000016 Valid Loss: 0.014194\n",
      "Época: 259/399 Train Loss: 0.000016 Valid Loss: 0.014202\n",
      "Época: 260/399 Train Loss: 0.000016 Valid Loss: 0.014211\n",
      "Época: 261/399 Train Loss: 0.000016 Valid Loss: 0.014215\n",
      "Época: 262/399 Train Loss: 0.000016 Valid Loss: 0.014219\n",
      "Época: 263/399 Train Loss: 0.000016 Valid Loss: 0.014230\n",
      "Época: 264/399 Train Loss: 0.000016 Valid Loss: 0.014236\n",
      "Época: 265/399 Train Loss: 0.000016 Valid Loss: 0.014244\n",
      "Época: 266/399 Train Loss: 0.000016 Valid Loss: 0.014251\n",
      "Época: 267/399 Train Loss: 0.000016 Valid Loss: 0.014258\n",
      "Época: 268/399 Train Loss: 0.000016 Valid Loss: 0.014265\n",
      "Época: 269/399 Train Loss: 0.000016 Valid Loss: 0.014274\n",
      "Época: 270/399 Train Loss: 0.000015 Valid Loss: 0.014281\n",
      "Época: 271/399 Train Loss: 0.000015 Valid Loss: 0.014290\n",
      "Época: 272/399 Train Loss: 0.000015 Valid Loss: 0.014290\n",
      "Época: 273/399 Train Loss: 0.000015 Valid Loss: 0.014299\n",
      "Época: 274/399 Train Loss: 0.000015 Valid Loss: 0.014309\n",
      "Época: 275/399 Train Loss: 0.000015 Valid Loss: 0.014314\n",
      "Época: 276/399 Train Loss: 0.000015 Valid Loss: 0.014320\n",
      "Época: 277/399 Train Loss: 0.000015 Valid Loss: 0.014329\n",
      "Época: 278/399 Train Loss: 0.000015 Valid Loss: 0.014338\n",
      "Época: 279/399 Train Loss: 0.000015 Valid Loss: 0.014339\n",
      "Época: 280/399 Train Loss: 0.000015 Valid Loss: 0.014349\n",
      "Época: 281/399 Train Loss: 0.000015 Valid Loss: 0.014352\n",
      "Época: 282/399 Train Loss: 0.000015 Valid Loss: 0.014360\n",
      "Época: 283/399 Train Loss: 0.000015 Valid Loss: 0.014369\n",
      "Época: 284/399 Train Loss: 0.000014 Valid Loss: 0.014375\n",
      "Época: 285/399 Train Loss: 0.000014 Valid Loss: 0.014382\n",
      "Época: 286/399 Train Loss: 0.000014 Valid Loss: 0.014385\n",
      "Época: 287/399 Train Loss: 0.000014 Valid Loss: 0.014391\n",
      "Época: 288/399 Train Loss: 0.000014 Valid Loss: 0.014394\n",
      "Época: 289/399 Train Loss: 0.000014 Valid Loss: 0.014402\n",
      "Época: 290/399 Train Loss: 0.000014 Valid Loss: 0.014410\n",
      "Época: 291/399 Train Loss: 0.000014 Valid Loss: 0.014419\n",
      "Época: 292/399 Train Loss: 0.000014 Valid Loss: 0.014428\n",
      "Época: 293/399 Train Loss: 0.000014 Valid Loss: 0.014433\n",
      "Época: 294/399 Train Loss: 0.000014 Valid Loss: 0.014439\n",
      "Época: 295/399 Train Loss: 0.000014 Valid Loss: 0.014446\n",
      "Época: 296/399 Train Loss: 0.000014 Valid Loss: 0.014451\n",
      "Época: 297/399 Train Loss: 0.000014 Valid Loss: 0.014453\n",
      "Época: 298/399 Train Loss: 0.000014 Valid Loss: 0.014460\n",
      "Época: 299/399 Train Loss: 0.000013 Valid Loss: 0.014465\n",
      "Época: 300/399 Train Loss: 0.000013 Valid Loss: 0.014475\n",
      "Época: 301/399 Train Loss: 0.000013 Valid Loss: 0.014481\n",
      "Época: 302/399 Train Loss: 0.000013 Valid Loss: 0.014484\n",
      "Época: 303/399 Train Loss: 0.000013 Valid Loss: 0.014490\n",
      "Época: 304/399 Train Loss: 0.000013 Valid Loss: 0.014496\n",
      "Época: 305/399 Train Loss: 0.000013 Valid Loss: 0.014504\n",
      "Época: 306/399 Train Loss: 0.000013 Valid Loss: 0.014515\n",
      "Época: 307/399 Train Loss: 0.000013 Valid Loss: 0.014517\n",
      "Época: 308/399 Train Loss: 0.000013 Valid Loss: 0.014525\n",
      "Época: 309/399 Train Loss: 0.000013 Valid Loss: 0.014532\n",
      "Época: 310/399 Train Loss: 0.000013 Valid Loss: 0.014536\n",
      "Época: 311/399 Train Loss: 0.000013 Valid Loss: 0.014540\n",
      "Época: 312/399 Train Loss: 0.000013 Valid Loss: 0.014547\n",
      "Época: 313/399 Train Loss: 0.000013 Valid Loss: 0.014555\n",
      "Época: 314/399 Train Loss: 0.000013 Valid Loss: 0.014560\n",
      "Época: 315/399 Train Loss: 0.000013 Valid Loss: 0.014564\n",
      "Época: 316/399 Train Loss: 0.000012 Valid Loss: 0.014570\n",
      "Época: 317/399 Train Loss: 0.000012 Valid Loss: 0.014578\n",
      "Época: 318/399 Train Loss: 0.000012 Valid Loss: 0.014584\n",
      "Época: 319/399 Train Loss: 0.000012 Valid Loss: 0.014591\n",
      "Época: 320/399 Train Loss: 0.000012 Valid Loss: 0.014598\n",
      "Época: 321/399 Train Loss: 0.000012 Valid Loss: 0.014604\n",
      "Época: 322/399 Train Loss: 0.000012 Valid Loss: 0.014607\n",
      "Época: 323/399 Train Loss: 0.000012 Valid Loss: 0.014613\n",
      "Época: 324/399 Train Loss: 0.000012 Valid Loss: 0.014615\n",
      "Época: 325/399 Train Loss: 0.000012 Valid Loss: 0.014623\n",
      "Época: 326/399 Train Loss: 0.000012 Valid Loss: 0.014632\n",
      "Época: 327/399 Train Loss: 0.000012 Valid Loss: 0.014636\n",
      "Época: 328/399 Train Loss: 0.000012 Valid Loss: 0.014640\n",
      "Época: 329/399 Train Loss: 0.000012 Valid Loss: 0.014647\n",
      "Época: 330/399 Train Loss: 0.000012 Valid Loss: 0.014652\n",
      "Época: 331/399 Train Loss: 0.000012 Valid Loss: 0.014660\n",
      "Época: 332/399 Train Loss: 0.000012 Valid Loss: 0.014665\n",
      "Época: 333/399 Train Loss: 0.000012 Valid Loss: 0.014671\n",
      "Época: 334/399 Train Loss: 0.000012 Valid Loss: 0.014673\n",
      "Época: 335/399 Train Loss: 0.000012 Valid Loss: 0.014679\n",
      "Época: 336/399 Train Loss: 0.000012 Valid Loss: 0.014683\n",
      "Época: 337/399 Train Loss: 0.000011 Valid Loss: 0.014687\n",
      "Época: 338/399 Train Loss: 0.000011 Valid Loss: 0.014694\n",
      "Época: 339/399 Train Loss: 0.000011 Valid Loss: 0.014699\n",
      "Época: 340/399 Train Loss: 0.000011 Valid Loss: 0.014705\n",
      "Época: 341/399 Train Loss: 0.000011 Valid Loss: 0.014709\n",
      "Época: 342/399 Train Loss: 0.000011 Valid Loss: 0.014718\n",
      "Época: 343/399 Train Loss: 0.000011 Valid Loss: 0.014722\n",
      "Época: 344/399 Train Loss: 0.000011 Valid Loss: 0.014728\n",
      "Época: 345/399 Train Loss: 0.000011 Valid Loss: 0.014732\n",
      "Época: 346/399 Train Loss: 0.000011 Valid Loss: 0.014736\n",
      "Época: 347/399 Train Loss: 0.000011 Valid Loss: 0.014744\n",
      "Época: 348/399 Train Loss: 0.000011 Valid Loss: 0.014750\n",
      "Época: 349/399 Train Loss: 0.000011 Valid Loss: 0.014754\n",
      "Época: 350/399 Train Loss: 0.000011 Valid Loss: 0.014760\n",
      "Época: 351/399 Train Loss: 0.000011 Valid Loss: 0.014767\n",
      "Época: 352/399 Train Loss: 0.000011 Valid Loss: 0.014771\n",
      "Época: 353/399 Train Loss: 0.000011 Valid Loss: 0.014774\n",
      "Época: 354/399 Train Loss: 0.000011 Valid Loss: 0.014778\n",
      "Época: 355/399 Train Loss: 0.000011 Valid Loss: 0.014784\n",
      "Época: 356/399 Train Loss: 0.000011 Valid Loss: 0.014789\n",
      "Época: 357/399 Train Loss: 0.000011 Valid Loss: 0.014794\n",
      "Época: 358/399 Train Loss: 0.000011 Valid Loss: 0.014802\n",
      "Época: 359/399 Train Loss: 0.000011 Valid Loss: 0.014805\n",
      "Época: 360/399 Train Loss: 0.000010 Valid Loss: 0.014809\n",
      "Época: 361/399 Train Loss: 0.000010 Valid Loss: 0.014815\n",
      "Época: 362/399 Train Loss: 0.000010 Valid Loss: 0.014817\n",
      "Época: 363/399 Train Loss: 0.000010 Valid Loss: 0.014827\n",
      "Época: 364/399 Train Loss: 0.000010 Valid Loss: 0.014831\n",
      "Época: 365/399 Train Loss: 0.000010 Valid Loss: 0.014835\n",
      "Época: 366/399 Train Loss: 0.000010 Valid Loss: 0.014842\n",
      "Época: 367/399 Train Loss: 0.000010 Valid Loss: 0.014847\n",
      "Época: 368/399 Train Loss: 0.000010 Valid Loss: 0.014853\n",
      "Época: 369/399 Train Loss: 0.000010 Valid Loss: 0.014855\n",
      "Época: 370/399 Train Loss: 0.000010 Valid Loss: 0.014861\n",
      "Época: 371/399 Train Loss: 0.000010 Valid Loss: 0.014865\n",
      "Época: 372/399 Train Loss: 0.000010 Valid Loss: 0.014870\n",
      "Época: 373/399 Train Loss: 0.000010 Valid Loss: 0.014873\n",
      "Época: 374/399 Train Loss: 0.000010 Valid Loss: 0.014883\n",
      "Época: 375/399 Train Loss: 0.000010 Valid Loss: 0.014887\n",
      "Época: 376/399 Train Loss: 0.000010 Valid Loss: 0.014891\n",
      "Época: 377/399 Train Loss: 0.000010 Valid Loss: 0.014897\n",
      "Época: 378/399 Train Loss: 0.000010 Valid Loss: 0.014901\n",
      "Época: 379/399 Train Loss: 0.000010 Valid Loss: 0.014906\n",
      "Época: 380/399 Train Loss: 0.000010 Valid Loss: 0.014908\n",
      "Época: 381/399 Train Loss: 0.000010 Valid Loss: 0.014916\n",
      "Época: 382/399 Train Loss: 0.000010 Valid Loss: 0.014922\n",
      "Época: 383/399 Train Loss: 0.000010 Valid Loss: 0.014924\n",
      "Época: 384/399 Train Loss: 0.000010 Valid Loss: 0.014928\n",
      "Época: 385/399 Train Loss: 0.000010 Valid Loss: 0.014934\n",
      "Época: 386/399 Train Loss: 0.000010 Valid Loss: 0.014939\n",
      "Época: 387/399 Train Loss: 0.000010 Valid Loss: 0.014944\n",
      "Época: 388/399 Train Loss: 0.000009 Valid Loss: 0.014950\n",
      "Época: 389/399 Train Loss: 0.000009 Valid Loss: 0.014953\n",
      "Época: 390/399 Train Loss: 0.000009 Valid Loss: 0.014956\n",
      "Época: 391/399 Train Loss: 0.000009 Valid Loss: 0.014963\n",
      "Época: 392/399 Train Loss: 0.000009 Valid Loss: 0.014967\n",
      "Época: 393/399 Train Loss: 0.000009 Valid Loss: 0.014974\n",
      "Época: 394/399 Train Loss: 0.000009 Valid Loss: 0.014977\n",
      "Época: 395/399 Train Loss: 0.000009 Valid Loss: 0.014982\n",
      "Época: 396/399 Train Loss: 0.000009 Valid Loss: 0.014984\n",
      "Época: 397/399 Train Loss: 0.000009 Valid Loss: 0.014990\n",
      "Época: 398/399 Train Loss: 0.000009 Valid Loss: 0.014995\n",
      "Época: 399/399 Train Loss: 0.000009 Valid Loss: 0.015000\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = 10e9\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    model.train()\n",
    "    for x_train, y_train in loader_train:\n",
    "        # Transforma a entrada para uma dimensão\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "\n",
    "        x_train = x_train.reshape(-1, 28*28)\n",
    "        # predict da rede\n",
    "        outputs = model(x_train)\n",
    "\n",
    "        # calcula a perda\n",
    "        batch_loss = criterion(outputs, y_train)\n",
    "\n",
    "        # zero, backpropagation, ajusta parâmetros pelo gradiente descendente\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        accumulated_loss += batch_loss.item()\n",
    "        run['train/batch_loss'].log(batch_loss)\n",
    "\n",
    "    train_loss = accumulated_loss / len(loader_train.dataset)\n",
    "    run['train/loss'].log(train_loss)\n",
    "    \n",
    "    # Laço de Validação, um a cada época.\n",
    "    accumulated_loss = 0\n",
    "    accumulated_accuracy = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_valid, y_valid in loader_valid:\n",
    "            x_valid = x_valid.to(device)\n",
    "            y_valid = y_valid.to(device)\n",
    "\n",
    "            # Transforma a entrada para uma dimensão\n",
    "            x_valid = x_valid.reshape(-1, 28*28)\n",
    "            # predict da rede\n",
    "            outputs = model(x_valid)\n",
    "\n",
    "            # calcula a perda\n",
    "            batch_loss = criterion(outputs, y_valid)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            # calcula a acurácia\n",
    "            batch_accuracy = (preds == y_valid).sum()\n",
    "            accumulated_loss += batch_loss\n",
    "            accumulated_accuracy += batch_accuracy\n",
    "\n",
    "    valid_loss = accumulated_loss / len(loader_valid.dataset)\n",
    "    run['valid/loss'].log(valid_loss)\n",
    "    run['valid/acuracy'].log(accumulated_accuracy / len(loader_valid.dataset))\n",
    "\n",
    "    print(f'Época: {i:d}/{n_epochs - 1:d} Train Loss: {train_loss:.6f} Valid Loss: {valid_loss:.6f}')\n",
    "\n",
    "    # Salvando o melhor modelo de acordo com a loss de validação\n",
    "    if valid_loss < best_valid_loss:\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        best_valid_loss = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PiuMsjYtQT2R",
    "outputId": "1d54f436-50b9-481a-8b27-68a19681e290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.576230525970459\n"
     ]
    }
   ],
   "source": [
    "print('Final loss:', batch_loss.item())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "05_05_Treino_Validação_MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "nav_menu": {
    "height": "318px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
