{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "Cópia de Aula 6 - Exercício - CIFAR 10 + CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jTyZCjzpFPBo"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3710jvsc74a57bd08ed6b52cc8d709cc5f65f43b3ffb99a31ba1f6a562ecd2294c0ff0768efdd52c",
      "display_name": "Python 3.7.10 64-bit ('pytorch': conda)"
    },
    "toc": {
      "nav_menu": {
        "height": "318px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3R2-_YuQT1r"
      },
      "source": [
        "# Treinamento de uma CNN no CIFAR 10\n",
        "\n",
        "**Nome: ** Vinicius Rodrigues Sanches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkfGTqMVQT1u"
      },
      "source": [
        "O objetivo deste exercício é implementar e treinar uma rede convolucional que obtenha a maior acurácia no dataset de teste do CIFAR 10.\n",
        "\n",
        "Podem definir sua própria rede ou utilizar redes pré-treinadas, como a Resnet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6tHIO8O-ePC"
      },
      "source": [
        "## Inicializando o Neptune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtKt7Ey7-htu",
        "outputId": "861e1a62-1e1a-43be-c472-afd7a983b8f0"
      },
      "source": [
        "! pip install neptune-client==0.9.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: neptune-client==0.9.1 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (7.1.2)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (1.3.0)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (0.18.2)\n",
            "Requirement already satisfied: websocket-client>=0.35.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (0.59.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (1.24.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (1.1.5)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (3.1.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (20.9)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (3.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (1.15.0)\n",
            "Requirement already satisfied: bravado in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (11.0.3)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (2.1.0)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (2.23.0)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client==0.9.1) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client==0.9.1) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client==0.9.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client==0.9.1) (2.8.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client==0.9.1) (4.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client==0.9.1) (2.4.7)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client==0.9.1) (1.6)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client==0.9.1) (5.17.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client==0.9.1) (3.13)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client==0.9.1) (3.17.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client==0.9.1) (3.7.4.3)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client==0.9.1) (1.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client==0.9.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client==0.9.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client==0.9.1) (2020.12.5)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client==0.9.1) (4.0.0)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.9.1) (0.2)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.9.1) (2.7.3)\n",
            "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.9.1) (2.6.0)\n",
            "Requirement already satisfied: rfc3987; extra == \"format\" in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.1) (1.3.8)\n",
            "Requirement already satisfied: strict-rfc3339; extra == \"format\" in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.1) (0.7)\n",
            "Requirement already satisfied: webcolors; extra == \"format\" in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.1) (1.11.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgE0kR_P-iea",
        "outputId": "822eae8b-eedd-459c-a299-f8716c07084a"
      },
      "source": [
        "import neptune.new as neptune\n",
        "\n",
        "# Insira seu api_token para logar os resultados do treino na sua conta do Neptune.\n",
        "# Como obter seu API token do Neptune:\n",
        "# https://docs.neptune.ai/administration/security-and-privacy/how-to-find-and-set-neptune-api-token\n",
        "\n",
        "run = neptune.init(project='vinicius-sanches/Aula6', api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2NjczZjk2MS1jZGE0LTQ4N2MtOTAwNi1kMGE1MjNiOGVkYjAifQ==')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://app.neptune.ai/vinicius-sanches/Aula6/e/AUL1-75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFNf4RPxQT1w"
      },
      "source": [
        "## Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fLUSHaCQT1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257cf6ae-1912-4e55-cf6e-8eb32ea76884"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "torch.manual_seed(123)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7fcb82c090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76PDhfUW6Afv"
      },
      "source": [
        "## Download do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0dEKCn-QT17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8030c59-908d-4e9a-9435-459caf4fd636"
      },
      "source": [
        "! mkdir ./data\n",
        "\n",
        "#transform = torchvision.transforms.Compose(\n",
        "#    [torchvision.transforms.ToTensor(),\n",
        "#     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "## Normalização Eduardo\n",
        "\n",
        "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "transform = torchvision.transforms.Compose([\n",
        "                    torchvision.transforms.ToTensor(),\n",
        "                    torchvision.transforms.Resize((224, 224)),\n",
        "                    normalize,\n",
        "        ])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                             download=True, transform=transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                            download=True, transform=transform)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./data’: File exists\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEMUsfJpQT11"
      },
      "source": [
        "## Dataset e dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9sXttHBTzCM"
      },
      "source": [
        "train_size = 40000\n",
        "val_size = 10000\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w52KGYlIQT2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "697f8d91-7445-49b5-dbc0-034d8ce84508"
      },
      "source": [
        "batch_size = 500\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print('Número de minibatches de trenamento:', len(train_dataloader))\n",
        "print('Número de minibatches de validação:', len(val_dataloader))\n",
        "print('Número de minibatches de teste:', len(test_dataloader))\n",
        "\n",
        "x_train, y_train = next(iter(train_dataloader))\n",
        "print(\"\\nDimensões dos dados de um minibatch:\", x_train.size())\n",
        "print(\"Valores mínimo e máximo dos pixels: \", torch.min(x_train), torch.max(x_train))\n",
        "print(\"Tipo dos dados das imagens:         \", type(x_train))\n",
        "print(\"Tipo das classes das imagens:       \", type(y_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de minibatches de trenamento: 80\n",
            "Número de minibatches de validação: 20\n",
            "Número de minibatches de teste: 20\n",
            "\n",
            "Dimensões dos dados de um minibatch: torch.Size([500, 3, 224, 224])\n",
            "Valores mínimo e máximo dos pixels:  tensor(-2.1179) tensor(2.6400)\n",
            "Tipo dos dados das imagens:          <class 'torch.Tensor'>\n",
            "Tipo das classes das imagens:        <class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yBXqGDPAPXM"
      },
      "source": [
        "# Use True, para usar o pytorch lightning original\n",
        "pl_original = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTyZCjzpFPBo"
      },
      "source": [
        "## Usando o Pytorch Lightining \"SuperLight\" (criado apenas para o curso).\n",
        "\n",
        "Criamos um Pytorch Lightning \"básico\" que esperamos ser mais didático que o original pois o código é facil de entender caso ocorra algum erro.\n",
        "\n",
        "As classes `LightningModule` e `Trainer` não precisam ser implementadas. Entretanto, para cada nova tarefa, uma classe que herda do `LightningModule` precisa ser definida e os seguintes métodos devem ser implementados:\n",
        "\n",
        " - \\_\\_init\\_\\_\n",
        " - forward\n",
        " - train_step\n",
        " - train_epoch_end\n",
        " - validation_step\n",
        " - validation_epoch_end\n",
        " - configure_optimizers\n",
        "\n",
        "Os métodos `test_step` e `test_epoch_end` devem ser implementados apenas se trainer.test() for chamado.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nKeJycqGEqL"
      },
      "source": [
        "import abc\n",
        "\n",
        "\n",
        "class LightningModule:\n",
        "    @abc.abstractmethod\n",
        "    def __init(self):\n",
        "        return\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def forward(self):\n",
        "        return\n",
        "\n",
        "    @abc.abstractmethod    \n",
        "    def training_step(self):\n",
        "        return \n",
        "\n",
        "    @abc.abstractmethod    \n",
        "    def training_epoch_end(self):\n",
        "        return \n",
        "\n",
        "    @abc.abstractmethod    \n",
        "    def validation_step(self):\n",
        "        return \n",
        "\n",
        "    @abc.abstractmethod    \n",
        "    def validation_epoch_end(self):\n",
        "        return \n",
        "\n",
        "    @abc.abstractmethod    \n",
        "    def test_step(self):\n",
        "        return \n",
        "\n",
        "    @abc.abstractmethod    \n",
        "    def test_epoch_end(self):\n",
        "        return\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def configure_optimizers(self):\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njy9UHW2FOmm"
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self, max_epochs: int, gpus: int = 1):\n",
        "        self.max_epochs = max_epochs\n",
        "        dev = \"cpu\" \n",
        "        if gpus > 0:\n",
        "            if torch.cuda.is_available(): \n",
        "                dev = \"cuda:0\"\n",
        "\n",
        "        print(f'Using {dev}')\n",
        "        self.device = torch.device(dev)\n",
        "\n",
        "    def fit(self, model, train_dataloader, val_dataloader=None):\n",
        "        assert isinstance(model, LightningModule)\n",
        "        best_valid_loss = 10e9\n",
        "        optimizers, _ = model.configure_optimizers()\n",
        "        optimizer = optimizers[0]\n",
        "        model.model.to(self.device)\n",
        "        \n",
        "        for i in range(self.max_epochs):\n",
        "            outputs = []\n",
        "            model.model.train()\n",
        "            for batch_idx, (x_train, y_train) in enumerate(train_dataloader):\n",
        "                x_train = x_train.to(self.device)\n",
        "                y_train = y_train.to(self.device)\n",
        "                output_dict = model.training_step((x_train, y_train), batch_idx)\n",
        "                loss = output_dict['loss']\n",
        "                # zero, backpropagation, ajusta parâmetros pelo gradiente descendente\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                outputs.append(output_dict)\n",
        "\n",
        "            model.training_epoch_end(outputs=outputs)\n",
        "            \n",
        "            # Laço de Validação, um a cada época.\n",
        "            if val_dataloader:\n",
        "                output_val_end = self.validate(model, val_dataloader)\n",
        "                print(f'Epoch {i} - {output_val_end[\"progress_bar\"]}')\n",
        "                # Salvando o melhor modelo de acordo com a loss de validação.\n",
        "                if output_val_end['valid_loss'] < best_valid_loss:\n",
        "                    torch.save(model.model.state_dict(), 'best_model.pt')\n",
        "                    best_valid_loss = output_val_end['valid_loss']\n",
        "\n",
        "    def validate(self, model, val_dataloader):\n",
        "        outputs = []\n",
        "        model.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (x, y) in enumerate(val_dataloader):\n",
        "                x = x.to(self.device)\n",
        "                y = y.to(self.device)\n",
        "                output_dict = model.validation_step((x, y), batch_idx)\n",
        "                outputs.append(output_dict)\n",
        "\n",
        "        output_dict = model.validation_epoch_end(outputs=outputs)\n",
        "        return output_dict\n",
        "\n",
        "    def test(self, model, test_dataloader):\n",
        "        outputs = []\n",
        "        model.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (x, y) in enumerate(test_dataloader):\n",
        "                x = x.to(self.device)\n",
        "                y = y.to(self.device)\n",
        "                output_dict = model.test_step((x, y), batch_idx)\n",
        "                outputs.append(output_dict)\n",
        "\n",
        "        output_dict = model.test_epoch_end(outputs=outputs)\n",
        "        return output_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smhZCgUPwp0o"
      },
      "source": [
        "if pl_original:\n",
        "    ! pip install pytorch_lightning\n",
        "    from pytorch_lightning import LightningModule, Trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQA9Zg7GQT2G"
      },
      "source": [
        "## Definindo o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8Eg4h_kQT2H"
      },
      "source": [
        "class Modelo(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Modelo, self).__init__()\n",
        "        # Defina seu modelo aqui.        \n",
        "        self.model = resnet50(pretrained=True)\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.model.fc = torch.nn.Sequential(\n",
        "               torch.nn.Linear(2048, 1024),\n",
        "               torch.nn.ReLU(inplace=True),\n",
        "               torch.nn.Linear(1024, 128),\n",
        "               torch.nn.ReLU(inplace=True),\n",
        "               torch.nn.Linear(128, 10),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        #self.dense = torch.nn.Sequential(\n",
        "            #torch.nn.Linear(32*32*3, 500),\n",
        "            #torch.nn.ReLU(),\n",
        "            #torch.nn.Linear(500, 10),\n",
        "\n",
        "        #    torch.nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "        #    torch.nn.ReLU(),\n",
        "        #    torch.nn.Conv2d(32, 128, kernel_size=3, stride=1, padding=1),\n",
        "        #    torch.nn.ReLU(),\n",
        "        #    torch.nn.MaxPool2d(2, 2), # output: 128 x 16 x 16\n",
        "\n",
        "        #    torch.nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "        #    torch.nn.ReLU(),\n",
        "        #    torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "        #    torch.nn.ReLU(),\n",
        "        #    torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "        #    torch.nn.ReLU(),\n",
        "        #    torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "        #    torch.nn.ReLU(),\n",
        "        #    torch.nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
        "        #    torch.nn.ReLU(),\n",
        "        #    torch.nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
        "\n",
        "        #    torch.nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "        #    torch.nn.ReLU(),\n",
        "        #    torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "        #    torch.nn.ReLU(),\n",
        "        #    torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "        #    torch.nn.ReLU(),\n",
        "        #    torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "        #    torch.nn.ReLU(),\n",
        "        #    torch.nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
        "\n",
        "         #   torch.nn.Flatten(), \n",
        "          #  torch.nn.Linear(256*4*4, 1024),\n",
        "           # torch.nn.ReLU(),\n",
        "            #torch.nn.Linear(1024,2048),\n",
        "            #torch.nn.Tanh(),\n",
        "           # torch.nn.ReLU(),\n",
        "           # torch.nn.Linear(2048,1024),\n",
        "            #torch.nn.ReLU(),\n",
        "            #torch.nn.Linear(1024,2048),\n",
        "            #torch.nn.ReLU(),\n",
        "            #torch.nn.Linear(2048,1024),\n",
        "            #torch.nn.ReLU(),\n",
        "            #torch.nn.Linear(1024, 512),\n",
        "            #torch.nn.ReLU(),\n",
        "            #torch.nn.Linear(512, 10),\n",
        "        #)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpTybLHCKVqw"
      },
      "source": [
        "## Criação do modelo Pytorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-9tbSvrKcSL"
      },
      "source": [
        "class LightningClassifier(LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hparams = hparams\n",
        "        self.criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "        # Note como a arquitetura esta dependente dos hiperparâmetros salvos.\n",
        "        self.model = Modelo()\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.model(x)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        return logits, preds\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        x, y = train_batch\n",
        "        #x = x.reshape(-1, 32*32*3)\n",
        "        # predict da rede\n",
        "        logits = self.model(x)\n",
        "\n",
        "        # calcula a perda\n",
        "        batch_losses = self.criterion(logits, y)\n",
        "        loss = batch_losses.mean()\n",
        "        run['train/batch_loss'].log(loss)\n",
        "\n",
        "        # O PL sempre espera um retorno nomeado 'loss' da training_step.\n",
        "        return {'loss': loss, 'batch_losses': batch_losses}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([output['batch_losses'] for output in outputs]).mean()\n",
        "\n",
        "        run['train/loss'].log(avg_loss)\n",
        "        return\n",
        "  \n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x, y = val_batch\n",
        "        \n",
        "        # Transforma a entrada para duas dimensões\n",
        "        #x = x.reshape(-1, 32*32*3)\n",
        "        # predict da rede\n",
        "        logits, preds = self.forward(x)\n",
        "\n",
        "        # calcula a perda\n",
        "        batch_losses = self.criterion(logits, y)\n",
        "        # calcula a acurácia\n",
        "        batch_accuracy = (preds == y)\n",
        "        \n",
        "        # Retornamos as losses do batch para podermos fazer a média no validation_epoch_end.\n",
        "        return {'batch_losses': batch_losses, 'batch_accuracy': batch_accuracy}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([output['batch_losses'] for output in outputs]).mean()\n",
        "        accuracy = torch.stack([output['batch_accuracy'] for output in outputs]).float().mean()\n",
        "\n",
        "        run['valid/loss'].log(avg_loss)\n",
        "        run['valid/acuracy'].log(accuracy)\n",
        "\n",
        "        metrics = {'valid_loss': avg_loss.item(), 'accuracy': accuracy.item()}\n",
        "        output =  {'progress_bar': metrics, 'valid_loss': avg_loss.item()}\n",
        "        return output\n",
        "  \n",
        "    def test_step(self, val_batch, batch_idx):\n",
        "        # A implementação deste método é opcional no Pytorch Lightning.\n",
        "        x, y = val_batch\n",
        "        \n",
        "        # Transforma a entrada para duas dimensões\n",
        "        #x = x.reshape(-1, 32*32*3)\n",
        "        # predict da rede\n",
        "        logits, preds = self.forward(x)\n",
        "\n",
        "        # calcula a perda\n",
        "        batch_losses = self.criterion(logits, y)\n",
        "        # calcula a acurácia\n",
        "        batch_accuracy = (preds == y)\n",
        "        \n",
        "        # Retornamos as losses do batch para podermos fazer a média no validation_epoch_end.\n",
        "        return {'batch_losses': batch_losses, 'batch_accuracy': batch_accuracy}\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        # A implementação deste método é opcional no Pytorch Lightning.\n",
        "        avg_loss = torch.stack([output['batch_losses'] for output in outputs]).mean()\n",
        "        accuracy = torch.stack([output['batch_accuracy'] for output in outputs]).float().mean()\n",
        "\n",
        "        run['valid/loss'].log(avg_loss)\n",
        "        run['valid/acuracy'].log(accuracy)\n",
        "        metrics = {'Test loss': avg_loss.item(), 'test accuracy': accuracy.item()}\n",
        "        output =  {'progress_bar': metrics}\n",
        "        return output\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Gradiente descendente\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hparams['learning_rate'])\n",
        "        # Aqui usamos um scheduler dummy pois o pytorch lightning original requer um.\n",
        "        scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 1.0)\n",
        "        return [optimizer], [scheduler]  # Forma de retorno para associar um otimizador a um scheduler."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqs2JhJoQT2L"
      },
      "source": [
        "## Inicialização dos parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZuYEkn_QT2M"
      },
      "source": [
        "hparams = {\n",
        "    'max_epochs': 10,\n",
        "    'learning_rate': 0.001\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmXarXeIQT2O"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5T_jZZPQT2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "956c726d-90f0-419c-8507-9bfc3558df0f"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "pl_model = LightningClassifier(hparams=hparams)\n",
        "trainer = Trainer(max_epochs=hparams['max_epochs'])\n",
        "trainer.fit(pl_model, train_dataloader, val_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda:0\n",
            "Epoch 0 - {'valid_loss': 0.6658190488815308, 'accuracy': 0.7680999636650085}\n",
            "Epoch 1 - {'valid_loss': 0.5999671816825867, 'accuracy': 0.795799970626831}\n",
            "Epoch 2 - {'valid_loss': 0.560926616191864, 'accuracy': 0.8050999641418457}\n",
            "Epoch 3 - {'valid_loss': 0.5577221512794495, 'accuracy': 0.8068999648094177}\n",
            "Epoch 4 - {'valid_loss': 0.5769901871681213, 'accuracy': 0.8069999814033508}\n",
            "Epoch 5 - {'valid_loss': 0.5344492793083191, 'accuracy': 0.8136000037193298}\n",
            "Epoch 6 - {'valid_loss': 0.5423210263252258, 'accuracy': 0.8127999901771545}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-13:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/neptune/new/internal/backends/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 229, in ping_run\n",
            "    self.leaderboard_client.api.ping(experimentId=str(run_uuid)).response().result\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/bravado/http_future.py\", line 239, in response\n",
            "    six.reraise(*sys.exc_info())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/six.py\", line 703, in reraise\n",
            "    raise value\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/bravado/http_future.py\", line 200, in response\n",
            "    swagger_result = self._get_swagger_result(incoming_response)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/bravado/http_future.py\", line 124, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/bravado/http_future.py\", line 303, in _get_swagger_result\n",
            "    self.request_config.response_callbacks,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/bravado/http_future.py\", line 334, in unmarshal_response\n",
            "    raise_on_unexpected(incoming_response)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/bravado/http_future.py\", line 408, in raise_on_unexpected\n",
            "    raise make_http_exception(response=http_response)\n",
            "bravado.exception.HTTPServiceUnavailable: 503 Service Temporarily Unavailable: <html>\r\n",
            "<head><title>503 Service Temporarily Unavailable</title></head>\r\n",
            "<body>\r\n",
            "<center><h1>503 Service Temporarily Unavailable</h1></center>\r\n",
            "<hr><center>nginx</center>\r\n",
            "</body>\r\n",
            "</html>\r\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/neptune/new/internal/threading/daemon.py\", line 46, in run\n",
            "    self.work()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/neptune/new/internal/utils/ping_background_job.py\", line 62, in work\n",
            "    self._run.ping()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/neptune/new/run.py\", line 94, in ping\n",
            "    self._backend.ping_run(self._uuid)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/neptune/new/internal/backends/utils.py\", line 92, in wrapper\n",
            "    raise NeptuneConnectionLostException() from last_exception\n",
            "neptune.new.exceptions.NeptuneConnectionLostException: \n",
            "\u001b[95m\n",
            "----NeptuneConnectionLostException---------------------------------------------------------\n",
            "\u001b[0m\n",
            "A connection to the Neptune server was lost.\n",
            "If you are using asynchronous (default) connection mode Neptune automatically switched to an offline mode and your data is being stored safely on the disk.\n",
            "You can upload it later using Neptune Command Line Interface:\n",
            "    \u001b[95mneptune sync -p workspace_name/project_name\u001b[0m\n",
            "\n",
            "What should I do?\n",
            "    - Check if your computer is connected to the internet.\n",
            "    - If your connection is unstable you can consider working using the offline mode:\n",
            "        \u001b[96mrun = neptune.init(mode=\"offline\")\u001b[0m\n",
            "        \n",
            "You can read in detail how it works and how to upload your data on the following doc pages:\n",
            "    - https://docs.neptune.ai/advanced-user-guides/connection-modes#offline\n",
            "    - https://docs.neptune.ai/advanced-user-guides/uploading-offline-data\n",
            "    \n",
            "You may also want to check the following docs pages:\n",
            "    - https://docs.neptune.ai/advanced-user-guides/connectivity-issues\n",
            "    - https://docs.neptune.ai/advanced-user-guides/connection-modes\n",
            "    \n",
            "\u001b[92mNeed help?\u001b[0m-> https://docs.neptune.ai/getting-started/getting-help\n",
            "\n",
            "\n",
            "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
            "Experiencing connection interruptions. Reestablishing communication with Neptune.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7 - {'valid_loss': 0.5421552062034607, 'accuracy': 0.8118999600410461}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Error occurred during asynchronous operation processing: Timestamp must be non-decreasing for series attribute: train/batch_loss. Invalid point: 2021-05-06T02:17:07.682Z\n",
            "Error occurred during asynchronous operation processing: Invalid point for string series: monitoring/stderr : Text longer than 1000 characters was truncated\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8 - {'valid_loss': 0.5111066102981567, 'accuracy': 0.8217999935150146}\n",
            "Epoch 9 - {'valid_loss': 0.5041409730911255, 'accuracy': 0.8269000053405762}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBGYUaQnIOTu"
      },
      "source": [
        "## Teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDaayYS8IRIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a023325-aae7-4721-afeb-491e786420dc"
      },
      "source": [
        "trainer.test(pl_model, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'progress_bar': {'Test loss': 0.5029945969581604,\n",
              "  'test accuracy': 0.8279999494552612}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTeXIQ83UsJt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}